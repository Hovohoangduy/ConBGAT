{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"UI9ulrJ9Qmti"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install python-dotenv"],"metadata":{"id":"X4NquqJcbSHd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -U attrdict3"],"metadata":{"id":"aIXqGB9YQ9QI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data processing"],"metadata":{"id":"0CILJ4NLOFvl"}},{"cell_type":"code","source":["from argparse import ArgumentParser\n","import os\n","from attrdict import AttrDict\n","import yaml\n","\n","def create_folder(folder_path : str) -> None:\n","    \"\"\"create a folder if not exists\n","\n","    Args:\n","        folder_path (str): path\n","    \"\"\"\n","    if not os.path.exists(folder_path):\n","        os.mkdir(folder_path)\n","\n","    return\n","\n","def get_config(name : str) -> AttrDict:\n","    \"\"\"get yaml config file\n","\n","    Args:\n","        name (str): yaml file name without extension\n","\n","    Returns:\n","        AttrDict: config\n","    \"\"\"\n","    with open(CONFIGS / f'{name}.yaml') as fileobj:\n","        config = AttrDict(yaml.safe_load(fileobj))\n","    return config\n","\n","def project_tree() -> None:\n","    \"\"\" Create the project tree folder\n","    \"\"\"\n","    create_folder(DATA)\n","    create_folder(OUTPUTS)\n","    create_folder(RUNS)\n","    create_folder(RESULTS)\n","    create_folder(TRAIN_SAMPLES)\n","    create_folder(TEST_SAMPLES)\n","    create_folder(CHECKPOINTS)\n","    return\n","\n","def set_preprocessing(args: ArgumentParser) -> None:\n","    \"\"\" Set preprocessings args\n","\n","    Args:\n","        args (ArgumentParser):\n","    \"\"\"\n","    with open(CONFIGS / 'base.yaml') as fileobj:\n","        cfg_preprocessing = dict(yaml.safe_load(fileobj))\n","    cfg_preprocessing['FEATURES']['add_geom'] = args.add_geom\n","    cfg_preprocessing['FEATURES']['add_embs'] = args.add_embs\n","    cfg_preprocessing['FEATURES']['add_hist'] = args.add_hist\n","    cfg_preprocessing['FEATURES']['add_visual'] = args.add_visual\n","    cfg_preprocessing['FEATURES']['add_eweights'] = args.add_eweights\n","    cfg_preprocessing['FEATURES']['num_polar_bins'] = args.num_polar_bins\n","    cfg_preprocessing['LOADER']['src_data'] = args.src_data\n","    cfg_preprocessing['GRAPHS']['data_type'] = args.data_type\n","    cfg_preprocessing['GRAPHS']['edge_type'] = args.edge_type\n","    cfg_preprocessing['GRAPHS']['node_granularity'] = args.node_granularity\n","\n","    with open(CONFIGS / 'preprocessing.yaml', 'w') as f:\n","        yaml.dump(cfg_preprocessing, f)\n","    return"],"metadata":{"id":"s_O4XGWFQ9OS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!unzip /content/drive/MyDrive/doc2graph-master/tutorial/dataset.zip -d /content/drive/MyDrive/doc2graph-master/tutorial/funsd"],"metadata":{"id":"4L_rsOBpe1ti"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tY4od4xPQeW_"},"outputs":[],"source":["from PIL import Image, ImageDraw\n","\n","\n","image_name = '/content/drive/MyDrive/doc2graph-master/tutorial/funsd/dataset/training_data/images/0000971160.png' #! change this to see different outputs from FUNSD, or pass your own image!\n","# 82491256.png\n","image_path = str(image_name)\n","image = Image.open(image_path).convert('RGB')\n","image"]},{"cell_type":"code","source":["!pip install easyocr"],"metadata":{"id":"KMlSMI-efd4X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install --upgrade pytesseract"],"metadata":{"id":"3wU40yuOiFeX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!sudo apt install tesseract-ocr"],"metadata":{"id":"WLx9MKIgjMNK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torchvision\n","import numpy as np\n","from scipy.optimize import linprog\n","import os\n","from PIL import ImageDraw, Image\n","import json\n","import pytesseract\n","from pytesseract import Output\n","\n","\n","def scale_back(r, w, h): return [int(r[0]*w),\n","                                 int(r[1]*h), int(r[2]*w), int(r[3]*h)]\n","\n","\n","def center(r): return ((r[0] + r[2]) / 2, (r[1] + r[3]) / 2)\n","\n","\n","def isIn(c, r):\n","    if c[0] < r[0] or c[0] > r[2]:\n","        return False\n","    elif c[1] < r[1] or c[1] > r[3]:\n","        return False\n","    else:\n","        return True\n","\n","\n","def match_pred_w_gt(bbox_preds: torch.Tensor, bbox_gts: torch.Tensor, links_pair: list):\n","    bbox_iou = torchvision.ops.box_iou(boxes1=bbox_preds, boxes2=bbox_gts)\n","    bbox_iou = bbox_iou.numpy()\n","\n","    A_ub = np.zeros(shape=(\n","        bbox_iou.shape[0] + bbox_iou.shape[1], bbox_iou.shape[0] * bbox_iou.shape[1]))\n","    for r in range(bbox_iou.shape[0]):\n","        st = r * bbox_iou.shape[1]\n","        A_ub[r, st:st + bbox_iou.shape[1]] = 1\n","    for j in range(bbox_iou.shape[1]):\n","        r = j + bbox_iou.shape[0]\n","        A_ub[r, j::bbox_iou.shape[1]] = 1\n","    b_ub = np.ones(shape=A_ub.shape[0])\n","\n","    assignaments_score = linprog(\n","        c=-bbox_iou.reshape(-1), A_ub=A_ub, b_ub=b_ub, bounds=(0, 1), method=\"highs-ds\")\n","    if not assignaments_score.success:\n","        print(\"Optimization FAILED\")\n","    assignaments_score = assignaments_score.x.reshape(bbox_iou.shape)\n","    assignaments_ids = assignaments_score.argmax(axis=1)\n","\n","    # matched\n","    opt_assignaments = {}\n","    for idx in range(assignaments_score.shape[0]):\n","        if (bbox_iou[idx, assignaments_ids[idx]] > 0.5) and (assignaments_score[idx, assignaments_ids[idx]] > 0.9):\n","            opt_assignaments[idx] = assignaments_ids[idx]\n","    # unmatched predictions\n","    false_positive = [idx for idx in range(\n","        bbox_preds.shape[0]) if idx not in opt_assignaments]\n","    # unmatched gts\n","    false_negative = [idx for idx in range(\n","        bbox_gts.shape[0]) if idx not in opt_assignaments.values()]\n","\n","    gt2pred = {v: k for k, v in opt_assignaments.items()}\n","    link_false_neg = []\n","    for link in links_pair:\n","        if link[0] in false_negative or link[1] in false_negative:\n","            link_false_neg.append(link)\n","\n","    if len(links_pair) != 0:\n","        rate = len(link_false_neg) / len(links_pair)\n","    else:\n","        rate = 0\n","    return {\"pred2gt\": opt_assignaments, \"gt2pred\": gt2pred, \"false_positive\": false_positive, \"false_negative\": false_negative, \"n_link_fn\": int(len(link_false_neg) / 2), \"link_loss\": rate, \"entity_loss\": len(false_positive) / (len(false_positive) + len(opt_assignaments.keys()))}\n","\n","\n","def get_objects(path, mode):\n","    # TODO given a document, apply OCR or Yolo to detect either words or entities.\n","    return\n","\n","\n","def load_predictions(path_preds, path_gts, path_images, debug=False):\n","    # TODO read txt file and pass bounding box to the other function.\n","\n","    boxs_preds = []\n","    boxs_gts = []\n","    links_gts = []\n","    labels_gts = []\n","    texts_ocr = []\n","    all_paths = []\n","\n","    for img in os.listdir(path_images):\n","        all_paths.append(os.path.join(path_images, img))\n","        w, h = Image.open(os.path.join(path_images, img)).size\n","        texts = pytesseract.image_to_data(Image.open(\n","            os.path.join(path_images, img)), output_type=Output.DICT)\n","        tp = []\n","        n_elements = len(texts['level'])\n","        for t in range(n_elements):\n","            if int(texts['conf'][t]) > 50 and texts['text'][t] != ' ':\n","                b = [texts['left'][t], texts['top'][t], texts['left'][t] +\n","                     texts['width'][t], texts['top'][t] + texts['height'][t]]\n","                tp.append([b, texts['text'][t]])\n","        texts_ocr.append(tp)\n","        preds_name = img.split(\".\")[0] + '.txt'\n","        with open(os.path.join(path_preds, preds_name), 'r') as preds:\n","            lines = preds.readlines()\n","            boxs = list()\n","            for line in lines:\n","                scaled = scale_back([float(c)\n","                                    for c in line[:-1].split(\" \")[1:]], w, h)\n","                sw, sh = scaled[2] / 2, scaled[3] / 2\n","                boxs.append([scaled[0] - sw, scaled[1] - sh,\n","                            scaled[0] + sw, scaled[1] + sh])\n","            boxs_preds.append(boxs)\n","\n","        gts_name = img.split(\".\")[0] + '.json'\n","        with open(os.path.join(path_gts, gts_name), 'r') as f:\n","            form = json.load(f)['form']\n","            boxs = list()\n","            pair_labels = []\n","            ids = []\n","            labels = []\n","            for elem in form:\n","                boxs.append([float(e) for e in elem['box']])\n","                ids.append(elem['id'])\n","                labels.append(elem['label'])\n","                [pair_labels.append(pair) for pair in elem['linking']]\n","\n","            for p, pair in enumerate(pair_labels):\n","                pair_labels[p] = [ids.index(pair[0]), ids.index(pair[1])]\n","\n","            boxs_gts.append(boxs)\n","            links_gts.append(pair_labels)\n","            labels_gts.append(labels)\n","\n","    all_links = []\n","    all_preds = []\n","    all_labels = []\n","    all_texts = []\n","    dropped_links = 0\n","    dropped_entity = 0\n","\n","    for p in range(len(boxs_preds)):\n","        d = match_pred_w_gt(torch.tensor(\n","            boxs_preds[p]), torch.tensor(boxs_gts[p]), links_gts[p])\n","        dropped_links += d['link_loss']\n","        dropped_entity += d['entity_loss']\n","        links = list()\n","\n","        for link in links_gts[p]:\n","            if link[0] in d['false_negative'] or link[1] in d['false_negative']:\n","                continue\n","            else:\n","                links.append([d['gt2pred'][link[0]], d['gt2pred'][link[1]]])\n","        all_links.append(links)\n","\n","        preds = []\n","        labels = []\n","        texts = []\n","        for b, box in enumerate(boxs_preds[p]):\n","            if b in d['false_positive']:\n","                preds.append(box)\n","                labels.append('other')\n","            else:\n","                gt_id = d['pred2gt'][b]\n","                preds.append(box)\n","                labels.append(labels_gts[p][gt_id])\n","\n","            text = ''\n","            for tocr in texts_ocr[p]:\n","                if isIn(center(tocr[0]), box):\n","                    text += tocr[1] + ' '\n","\n","            texts.append(text)\n","\n","        all_preds.append(preds)\n","        all_labels.append(labels)\n","        all_texts.append(texts)\n","    print(dropped_links / len(boxs_preds), dropped_entity / len(boxs_preds))\n","\n","    if debug:\n","        # random.seed(35)\n","        # rand_idx = random.randint(0, len(os.listdir(path_images)))\n","        print(all_texts[0])\n","        rand_idx = 0\n","        img = Image.open(os.path.join(path_images, os.listdir(\n","            path_images)[rand_idx])).convert('RGB')\n","        draw = ImageDraw.Draw(img)\n","\n","        rand_boxs_preds = boxs_preds[rand_idx]\n","        rand_boxs_gts = boxs_gts[rand_idx]\n","\n","        for box in rand_boxs_gts:\n","            draw.rectangle(box, outline='blue', width=3)\n","        for box in rand_boxs_preds:\n","            draw.rectangle(box, outline='red', width=3)\n","\n","        d = match_pred_w_gt(torch.tensor(rand_boxs_preds),\n","                            torch.tensor(rand_boxs_gts), links_gts[rand_idx])\n","        print(d)\n","        for idx in d['pred2gt'].keys():\n","            draw.rectangle(rand_boxs_preds[idx], outline='green', width=3)\n","\n","        link_true_pos = list()\n","        link_false_neg = list()\n","        for link in links_gts[rand_idx]:\n","            if link[0] in d['false_negative'] or link[1] in d['false_negative']:\n","                link_false_neg.append(link)\n","                start = rand_boxs_gts[link[0]]\n","                end = rand_boxs_gts[link[1]]\n","                draw.line((center(start), center(end)), fill='red', width=3)\n","            else:\n","                link_true_pos.append(link)\n","                start = rand_boxs_preds[d['gt2pred'][link[0]]]\n","                end = rand_boxs_preds[d['gt2pred'][link[1]]]\n","                draw.line((center(start), center(end)), fill='green', width=3)\n","\n","        precision = 0\n","        recall = 0\n","        for idx, gt in enumerate(boxs_gts):\n","            d = match_pred_w_gt(torch.tensor(\n","                boxs_preds[idx]), torch.tensor(gt), links_gts[rand_idx])\n","            bbox_true_positive = len(d[\"pred2gt\"])\n","            p = bbox_true_positive / \\\n","                (bbox_true_positive + len(d[\"false_positive\"]))\n","            r = bbox_true_positive / \\\n","                (bbox_true_positive + len(d[\"false_negative\"]))\n","            # f1 += (2 * p * r) / (p + r)\n","            precision += p\n","            recall += r\n","\n","        precision = precision / len(boxs_gts)\n","        recall = recall / len(boxs_gts)\n","        f1 = (2 * precision * recall) / (precision + recall)\n","        # print(f1, precision, recall)\n","\n","        img.save('prova.png')\n","\n","    return all_paths, all_preds, all_links, all_labels, all_texts\n","\n","# if __name__ == \"__main__\":\n","#     path_preds = '/content/drive/MyDrive/doc2graph-master/tutorial/funsd'\n","#     path_images = '/content/drive/MyDrive/doc2graph-master/tutorial/funsd/dataset/training_data/images'\n","#     path_gts = '/content/drive/MyDrive/doc2graph-master/tutorial/funsd/dataset/training_data/annotations'\n","#     load_predictions(path_preds, path_gts, path_images, debug=True)"],"metadata":{"id":"P6GpArXjfd6l"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5NRh_i7vQeW_"},"outputs":[],"source":["import easyocr\n","\n","reader = easyocr.Reader(['en']) #! support multilingual!\n","\n","def apply_ocr(path):\n","    result = reader.readtext(path, paragraph=True)\n","    boxs, texts = list(), list()\n","\n","    # transform the OCR result in a handle format\n","    for r in result:\n","        box = [int(r[0][0][0]), int(r[0][0][1]), int(r[0][2][0]), int(r[0][2][1])]\n","        boxs.append(box)\n","        texts.append(r[1])\n","\n","    return boxs, texts\n","\n","def draw_results(img, boxs, links):\n","    draw = ImageDraw.Draw(img)\n","\n","    for box in boxs:\n","        draw.rectangle(box, outline='blue', width=3)\n","\n","    if links:\n","        for idx in range(len(links['src'])):\n","            key_center = center(boxs[links['src'][idx]])\n","            value_center = center(boxs[links['dst'][idx]])\n","            draw.line((key_center, value_center), fill='violet', width=3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dO_K2bV7QeW_"},"outputs":[],"source":["#! get text boxes and contents\n","boxs, texts = apply_ocr(image_path)\n","draw_results(image, boxs, [])\n","image"]},{"cell_type":"code","source":["from math import sqrt\n","from typing import Tuple\n","import cv2\n","import numpy as np\n","import torch\n","import math\n","\n","def polar(rect_src : list, rect_dst : list) -> Tuple[int, int]:\n","    \"\"\"Compute distance and angle from src to dst bounding boxes (poolar coordinates considering the src as the center)\n","    Args:\n","        rect_src (list) : source rectangle coordinates\n","        rect_dst (list) : destination rectangle coordinates\n","\n","    Returns:\n","        tuple (ints): distance and angle\n","    \"\"\"\n","\n","    # check relative position\n","    left = (rect_dst[2] - rect_src[0]) <= 0\n","    bottom = (rect_src[3] - rect_dst[1]) <= 0\n","    right = (rect_src[2] - rect_dst[0]) <= 0\n","    top = (rect_dst[3] - rect_src[1]) <= 0\n","\n","    vp_intersect = (rect_src[0] <= rect_dst[2] and rect_dst[0] <= rect_src[2]) # True if two rects \"see\" each other vertically, above or under\n","    hp_intersect = (rect_src[1] <= rect_dst[3] and rect_dst[1] <= rect_src[3]) # True if two rects \"see\" each other horizontally, right or left\n","    rect_intersect = vp_intersect and hp_intersect\n","\n","    center = lambda rect: ((rect[2]+rect[0])/2, (rect[3]+rect[1])/2)\n","\n","    # evaluate reciprocal position\n","    sc = center(rect_src)\n","    ec = center(rect_dst)\n","    new_ec = (ec[0] - sc[0], ec[1] - sc[1])\n","    angle = int(math.degrees(math.atan2(new_ec[1], new_ec[0])) % 360)\n","\n","    if rect_intersect:\n","        return 0, angle\n","    elif top and left:\n","        a, b = (rect_dst[2] - rect_src[0]), (rect_dst[3] - rect_src[1])\n","        return int(sqrt(a**2 + b**2)), angle\n","    elif left and bottom:\n","        a, b = (rect_dst[2] - rect_src[0]), (rect_dst[1] - rect_src[3])\n","        return int(sqrt(a**2 + b**2)), angle\n","    elif bottom and right:\n","        a, b = (rect_dst[0] - rect_src[2]), (rect_dst[1] - rect_src[3])\n","        return int(sqrt(a**2 + b**2)), angle\n","    elif right and top:\n","        a, b = (rect_dst[0] - rect_src[2]), (rect_dst[3] - rect_src[1])\n","        return int(sqrt(a**2 + b**2)), angle\n","    elif left:\n","        return (rect_src[0] - rect_dst[2]), angle\n","    elif right:\n","        return (rect_dst[0] - rect_src[2]), angle\n","    elif bottom:\n","        return (rect_dst[1] - rect_src[3]), angle\n","    elif top:\n","        return (rect_src[1] - rect_dst[3]), angle\n","\n","def transform_image(img_path : str, scale_image=1.0) -> torch.Tensor:\n","    \"\"\" Transform image to torch.Tensor\n","\n","    Args:\n","        img_path (str) : where the image is stored\n","        scale_image (float) : how much scale the image\n","    \"\"\"\n","\n","    np_img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n","    width = int(np_img.shape[1] * scale_image)\n","    height = int(np_img.shape[0] * scale_image)\n","    new_size = (width, height)\n","    np_img = cv2.resize(np_img,new_size)\n","    img = cv2.cvtColor(np_img, cv2.COLOR_BGR2GRAY)\n","    img = img[None,None,:,:]\n","    img = img.astype(np.float32)\n","    img = torch.from_numpy(img)\n","    img = 1.0 - img / 128.0\n","\n","    return img\n","\n","def get_histogram(contents : list) -> list:\n","    \"\"\"Create histogram of content given a text.\n","\n","    Args;\n","        contents (list)\n","\n","    Returns:\n","        list of [x, y, z] - 3-dimension list with float values summing up to 1 where:\n","            - x is the % of literals inside the text\n","            - y is the % of numbers inside the text\n","            - z is the % of other symbols i.e. @, #, .., inside the text\n","    \"\"\"\n","\n","    c_histograms = list()\n","\n","    for token in contents:\n","        num_symbols = 0 # all\n","        num_literals = 0 # A, B etc.\n","        num_figures = 0 # 1, 2, etc.\n","        num_others = 0 # !, @, etc.\n","\n","        histogram = [0.0000, 0.0000, 0.0000, 0.0000]\n","\n","        for symbol in token.replace(\" \", \"\"):\n","            if symbol.isalpha():\n","                num_literals += 1\n","            elif symbol.isdigit():\n","                num_figures += 1\n","            else:\n","                num_others += 1\n","            num_symbols += 1\n","\n","        if num_symbols != 0:\n","            histogram[0] = num_literals / num_symbols\n","            histogram[1] = num_figures / num_symbols\n","            histogram[2] = num_others / num_symbols\n","\n","            # keep sum 1 after truncate\n","            if sum(histogram) != 1.0:\n","                diff = 1.0 - sum(histogram)\n","                m = max(histogram) + diff\n","                histogram[histogram.index(max(histogram))] = m\n","\n","        # if symbols not recognized at all or empty, sum everything at 1 in the last\n","        if histogram[0:3] == [0.0,0.0,0.0]:\n","            histogram[3] = 1.0\n","\n","        c_histograms.append(histogram)\n","\n","    return c_histograms\n","\n","def to_bin(dist :int, angle : int, b=8) -> torch.Tensor:\n","    \"\"\" Discretize the space into equal \"bins\": return a distance and angle into a number between 0 and 1.\n","\n","    Args:\n","        dist (int): distance in terms of pixel, given by \"polar()\" util function\n","        angle (int): angle between 0 and 360, given by \"polar()\" util function\n","        b (int): number of bins, MUST be power of 2\n","\n","    Returns:\n","        torch.Tensor: new distance and angle (binary encoded)\n","\n","    \"\"\"\n","    def isPowerOfTwo(x):\n","        return (x and (not(x & (x - 1))) )\n","\n","    # dist\n","    assert isPowerOfTwo(b)\n","    m = max(dist) / b\n","    new_dist = []\n","    for d in dist:\n","        bin = int(d / m)\n","        if bin >= b: bin = b - 1\n","        bin = [int(x) for x in list('{0:0b}'.format(bin))]\n","        while len(bin) < sqrt(b): bin.insert(0, 0)\n","        new_dist.append(bin)\n","\n","    # angle\n","    amplitude = 360 / b\n","    new_angle = []\n","    for a in angle:\n","        bin = (a - amplitude / 2)\n","        bin = int(bin / amplitude)\n","        bin = [int(x) for x in list('{0:0b}'.format(bin))]\n","        while len(bin) < sqrt(b): bin.insert(0, 0)\n","        new_angle.append(bin)\n","\n","    return torch.cat([torch.tensor(new_dist, dtype=torch.float32), torch.tensor(new_angle, dtype=torch.float32)], dim=1)"],"metadata":{"id":"RLU6M1WXpETH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install dgl"],"metadata":{"id":"oSGBm_3nvjKz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","import os\n","from PIL import Image, ImageDraw\n","from typing import Tuple\n","import torch\n","import dgl\n","import random\n","import numpy as np\n","from tqdm import tqdm\n","import xml.etree.ElementTree as ET\n","import easyocr\n","\n","\n","class GraphBuilder():\n","\n","    def __init__(self):\n","        self.cfg_preprocessing = get_config('base')\n","        self.edge_type = self.cfg_preprocessing.GRAPHS.edge_type\n","        self.data_type = self.cfg_preprocessing.GRAPHS.data_type\n","        self.node_granularity = self.cfg_preprocessing.GRAPHS.node_granularity\n","        random.seed = 42\n","        return\n","\n","    def get_graph(self, src_path : str, src_data : str) -> Tuple[list, list, list, list]:\n","        \"\"\" Given the source, it returns a graph\n","\n","        Args:\n","            src_path (str) : path to source data\n","            src_data (str) : either FUNSD, PAU or CUSTOM\n","\n","        Returns:\n","            tuple (lists) : graphs, nodes and edge labels, features\n","        \"\"\"\n","\n","        if src_data == 'FUNSD':\n","            return self.__fromFUNSD(src_path)\n","        elif src_data == 'PAU':\n","            return self.__fromPAU(src_path)\n","        elif src_data == 'CUSTOM':\n","            if self.data_type == 'img':\n","                return self.__fromIMG(src_path)\n","            elif self.data_type == 'pdf':\n","                return self.__fromPDF()\n","            else:\n","                raise Exception('GraphBuilder exception: data type invalid. Choose from [\"img\", \"pdf\"]')\n","        else:\n","            raise Exception('GraphBuilder exception: source data invalid. Choose from [\"FUNSD\", \"PAU\", \"CUSTOM\"]')\n","\n","    def balance_edges(self, g : dgl.DGLGraph, cls=None ) -> dgl.DGLGraph:\n","        \"\"\" if cls (class) is not None, but an integer instead, balance that class to be equal to the sum of the other classes\n","\n","        Args:\n","            g (DGLGraph) : a DGL graph\n","            cls (int) : class number, if any\n","\n","        Returns:\n","            g (DGLGraph) : the new balanced graph\n","        \"\"\"\n","\n","        edge_targets = g.edata['label']\n","        u, v = g.all_edges(form='uv')\n","        edges_list = list()\n","        for e in zip(u.tolist(), v.tolist()):\n","            edges_list.append([e[0], e[1]])\n","\n","        if type(cls) is int:\n","            to_remove = (edge_targets == cls)\n","            indices_to_remove = to_remove.nonzero().flatten().tolist()\n","\n","            for _ in range(int((edge_targets != cls).sum()/2)):\n","                indeces_to_save = [random.choice(indices_to_remove)]\n","                edge = edges_list[indeces_to_save[0]]\n","\n","                for index in sorted(indeces_to_save, reverse=True):\n","                    del indices_to_remove[indices_to_remove.index(index)]\n","\n","            indices_to_remove = torch.flatten(torch.tensor(indices_to_remove, dtype=torch.int32))\n","            g = dgl.remove_edges(g, indices_to_remove)\n","            return g\n","\n","        else:\n","            raise Exception(\"Select a class to balance (an integer ranging from 0 to num_edge_classes).\")\n","\n","    def get_info(self):\n","        \"\"\" returns graph information\n","        \"\"\"\n","        print(f\"-> edge type: {self.edge_type}\")\n","\n","    def fully_connected(self, ids : list) -> Tuple[list, list]:\n","        \"\"\" create fully connected graph\n","\n","        Args:\n","            ids (list) : list of node indices\n","\n","        Returns:\n","            u, v (lists) : lists of indices\n","        \"\"\"\n","        u, v = list(), list()\n","        for id in ids:\n","            u.extend([id for i in range(len(ids)) if i != id])\n","            v.extend([i for i in range(len(ids)) if i != id])\n","        return u, v\n","\n","    def knn_connection(self, size : tuple, bboxs : list, k = 10) -> Tuple[list, list]:\n","        \"\"\" Given a list of bounding boxes, find for each of them their k nearest ones.\n","\n","        Args:\n","            size (tuple) : width and height of the image\n","            bboxs (list) : list of bounding box coordinates\n","            k (int) : k of the knn algorithm\n","\n","        Returns:\n","            u, v (lists) : lists of indices\n","        \"\"\"\n","\n","        edges = []\n","        width, height = size[0], size[1]\n","\n","        # creating projections\n","        vertical_projections = [[] for i in range(width)]\n","        horizontal_projections = [[] for i in range(height)]\n","        for node_index, bbox in enumerate(bboxs):\n","            for hp in range(bbox[0], bbox[2]):\n","                if hp >= width: hp = width - 1\n","                vertical_projections[hp].append(node_index)\n","            for vp in range(bbox[1], bbox[3]):\n","                if vp >= height: vp = height - 1\n","                horizontal_projections[vp].append(node_index)\n","\n","        def bound(a, ori=''):\n","            if a < 0 : return 0\n","            elif ori == 'h' and a > height: return height\n","            elif ori == 'w' and a > width: return width\n","            else: return a\n","\n","        for node_index, node_bbox in enumerate(bboxs):\n","            neighbors = [] # collect list of neighbors\n","            window_multiplier = 2 # how much to look around bbox\n","            wider = (node_bbox[2] - node_bbox[0]) > (node_bbox[3] - node_bbox[1]) # if bbox wider than taller\n","\n","            ### finding neighbors ###\n","            while(len(neighbors) < k and window_multiplier < 100): # keep enlarging the window until at least k bboxs are found or window too big\n","                vertical_bboxs = []\n","                horizontal_bboxs = []\n","                neighbors = []\n","\n","                if wider:\n","                    h_offset = int((node_bbox[2] - node_bbox[0]) * window_multiplier/4)\n","                    v_offset = int((node_bbox[3] - node_bbox[1]) * window_multiplier)\n","                else:\n","                    h_offset = int((node_bbox[2] - node_bbox[0]) * window_multiplier)\n","                    v_offset = int((node_bbox[3] - node_bbox[1]) * window_multiplier/4)\n","\n","                window = [bound(node_bbox[0] - h_offset),\n","                        bound(node_bbox[1] - v_offset),\n","                        bound(node_bbox[2] + h_offset, 'w'),\n","                        bound(node_bbox[3] + v_offset, 'h')]\n","\n","                [vertical_bboxs.extend(d) for d in vertical_projections[window[0]:window[2]]]\n","                [horizontal_bboxs.extend(d) for d in horizontal_projections[window[1]:window[3]]]\n","\n","                for v in set(vertical_bboxs):\n","                    for h in set(horizontal_bboxs):\n","                        if v == h: neighbors.append(v)\n","\n","                window_multiplier += 1 # enlarge the window\n","\n","            ### finding k nearest neighbors ###\n","            neighbors = list(set(neighbors))\n","            if node_index in neighbors:\n","                neighbors.remove(node_index)\n","            neighbors_distances = [polar(node_bbox, bboxs[n])[0] for n in neighbors]\n","            for sd_num, sd_idx in enumerate(np.argsort(neighbors_distances)):\n","                if sd_num < k:\n","                    if [node_index, neighbors[sd_idx]] not in edges and [neighbors[sd_idx], node_index] not in edges:\n","                        edges.append([neighbors[sd_idx], node_index])\n","                        edges.append([node_index, neighbors[sd_idx]])\n","                else: break\n","\n","        return [e[0] for e in edges], [e[1] for e in edges]\n","\n","    def __fromIMG(self, paths : list):\n","\n","        graphs, node_labels, edge_labels = list(), list(), list()\n","        features = {'paths': paths, 'texts': [], 'boxs': []}\n","\n","        for path in paths:\n","            reader = easyocr.Reader(['en']) #! TODO: in the future, handle multilanguage!\n","            result = reader.readtext(path, paragraph=True)\n","            img = Image.open(path).convert('RGB')\n","            draw = ImageDraw.Draw(img)\n","            boxs, texts = list(), list()\n","\n","            for r in result:\n","                box = [int(r[0][0][0]), int(r[0][0][1]), int(r[0][2][0]), int(r[0][2][1])]\n","                draw.rectangle(box, outline='red', width=3)\n","                boxs.append(box)\n","                texts.append(r[1])\n","\n","            features['boxs'].append(boxs)\n","            features['texts'].append(texts)\n","            img.save('prova.png')\n","\n","            if self.edge_type == 'fully':\n","                u, v = self.fully_connected(range(len(boxs)))\n","            elif self.edge_type == 'knn':\n","                u,v = self.knn_connection(Image.open(path).size, boxs)\n","            else:\n","                raise Exception('Other edge types still under development.')\n","\n","            g = dgl.graph((torch.tensor(u), torch.tensor(v)), num_nodes=len(boxs), idtype=torch.int32)\n","            graphs.append(g)\n","\n","        return graphs, node_labels, edge_labels, features\n","\n","    def __fromPDF():\n","        #TODO: dev from PDF import of graphs\n","        return\n","\n","    def __fromPAU(self, src: str) -> Tuple[list, list, list, list]:\n","        \"\"\" build graphs from Pau Riba's dataset\n","\n","        Args:\n","            src (str) : path to where data is stored\n","\n","        Returns:\n","            tuple (lists) : graphs, nodes and edge labels, features\n","        \"\"\"\n","\n","        graphs, node_labels, edge_labels = list(), list(), list()\n","        features = {'paths': [], 'texts': [], 'boxs': []}\n","\n","        for image in tqdm(os.listdir(src), desc='Creating graphs'):\n","            if not image.endswith('tif'): continue\n","\n","            img_name = image.split('.')[0]\n","            file_gt = img_name + '_gt.xml'\n","            file_ocr = img_name + '_ocr.xml'\n","\n","            if not os.path.isfile(os.path.join(src, file_gt)) or not os.path.isfile(os.path.join(src, file_ocr)): continue\n","            features['paths'].append(os.path.join(src, image))\n","\n","            # DOCUMENT REGIONS\n","            root = ET.parse(os.path.join(src, file_gt)).getroot()\n","            regions = []\n","            for parent in root:\n","                if parent.tag.split(\"}\")[1] == 'Page':\n","                    for child in parent:\n","                        region_label = child[0].attrib['value']\n","                        region_bbox = [int(child[1].attrib['points'].split(\" \")[0].split(\",\")[0].split(\".\")[0]),\n","                                    int(child[1].attrib['points'].split(\" \")[1].split(\",\")[1].split(\".\")[0]),\n","                                    int(child[1].attrib['points'].split(\" \")[2].split(\",\")[0].split(\".\")[0]),\n","                                    int(child[1].attrib['points'].split(\" \")[3].split(\",\")[1].split(\".\")[0])]\n","                        regions.append([region_label, region_bbox])\n","\n","            # DOCUMENT TOKENS\n","            root = ET.parse(os.path.join(src, file_ocr)).getroot()\n","            tokens_bbox = []\n","            tokens_text = []\n","            nl = []\n","            center = lambda rect: ((rect[2]+rect[0])/2, (rect[3]+rect[1])/2)\n","            for parent in root:\n","                if parent.tag.split(\"}\")[1] == 'Page':\n","                    for child in parent:\n","                        if child.tag.split(\"}\")[1] == 'TextRegion':\n","                            for elem in child:\n","                                if elem.tag.split(\"}\")[1] == 'TextLine':\n","                                    for word in elem:\n","                                        if word.tag.split(\"}\")[1] == 'Word':\n","                                            word_bbox = [int(word[0].attrib['points'].split(\" \")[0].split(\",\")[0].split(\".\")[0]),\n","                                                        int(word[0].attrib['points'].split(\" \")[1].split(\",\")[1].split(\".\")[0]),\n","                                                        int(word[0].attrib['points'].split(\" \")[2].split(\",\")[0].split(\".\")[0]),\n","                                                        int(word[0].attrib['points'].split(\" \")[3].split(\",\")[1].split(\".\")[0])]\n","                                            word_text = word[1][0].text\n","                                            c = center(word_bbox)\n","                                            for reg in regions:\n","                                                r = reg[1]\n","                                                if r[0] < c[0] < r[2] and r[1] < c[1] < r[3]:\n","                                                    word_label = reg[0]\n","                                                    break\n","                                            tokens_bbox.append(word_bbox)\n","                                            tokens_text.append(word_text)\n","                                            nl.append(word_label)\n","\n","            features['boxs'].append(tokens_bbox)\n","            features['texts'].append(tokens_text)\n","            node_labels.append(nl)\n","\n","            # getting edges\n","            if self.edge_type == 'fully':\n","                u, v = self.fully_connected(range(len(tokens_bbox)))\n","            elif self.edge_type == 'knn':\n","                u,v = self.knn_connection(Image.open(os.path.join(src, image)).size, tokens_bbox)\n","            else:\n","                raise Exception('Other edge types still under development.')\n","\n","            el = list()\n","            for e in zip(u, v):\n","                if (nl[e[0]] == nl[e[1]]) and (nl[e[0]] == 'positions' or nl[e[0]] == 'total'):\n","                    el.append('table')\n","                else: el.append('none')\n","            edge_labels.append(el)\n","\n","            g = dgl.graph((torch.tensor(u), torch.tensor(v)), num_nodes=len(tokens_bbox), idtype=torch.int32)\n","            graphs.append(g)\n","\n","        return graphs, node_labels, edge_labels, features\n","\n","    def __fromFUNSD(self, src : str) -> Tuple[list, list, list, list]:\n","        \"\"\"Parsing FUNSD annotation files\n","\n","        Args:\n","            src (str) : path to where data is stored\n","\n","        Returns:\n","            tuple (lists) : graphs, nodes and edge labels, features\n","        \"\"\"\n","\n","        graphs, node_labels, edge_labels = list(), list(), list()\n","        features = {'paths': [], 'texts': [], 'boxs': []}\n","        # justOne = random.choice(os.listdir(os.path.join(src, 'adjusted_annotations'))).split(\".\")[0]\n","\n","        if self.node_granularity[0] == 'gt':\n","            for file in tqdm(os.listdir(os.path.join(src, 'adjusted_annotations')), desc='Creating graphs - GT'):\n","\n","                img_name = f'{file.split(\".\")[0]}.png'\n","                img_path = os.path.join(src, 'images', img_name)\n","                features['paths'].append(img_path)\n","\n","                with open(os.path.join(src, 'adjusted_annotations', file), 'r') as f:\n","                    form = json.load(f)['form']\n","\n","                # getting infos\n","                boxs, texts, ids, nl = list(), list(), list(), list()\n","                pair_labels = list()\n","\n","                for elem in form:\n","                    boxs.append(elem['box'])\n","                    texts.append(elem['text'])\n","                    nl.append(elem['label'])\n","                    ids.append(elem['id'])\n","                    [pair_labels.append(pair) for pair in elem['linking']]\n","\n","                for p, pair in enumerate(pair_labels):\n","                    pair_labels[p] = [ids.index(pair[0]), ids.index(pair[1])]\n","\n","                node_labels.append(nl)\n","                features['texts'].append(texts)\n","                features['boxs'].append(boxs)\n","\n","                # getting edges\n","                if self.edge_type[0] == 'fully':\n","                    u, v = self.fully_connected(range(len(boxs)))\n","                elif self.edge_type[0] == 'knn':\n","                    u,v = self.knn_connection(Image.open(img_path).size, boxs)\n","                else:\n","                    raise Exception('GraphBuilder exception: Other edge types still under development.')\n","\n","                el = list()\n","                for e in zip(u, v):\n","                    edge = [e[0], e[1]]\n","                    if edge in pair_labels: el.append('pair')\n","                    else: el.append('none')\n","                edge_labels.append(el)\n","\n","                # creating graph\n","                g = dgl.graph((torch.tensor(u), torch.tensor(v)), num_nodes=len(boxs), idtype=torch.int32)\n","                graphs.append(g)\n","\n","            #! DEBUG PURPOSES TO VISUALIZE RANDOM GRAPH IMAGE FROM DATASET\n","            if False:\n","                if justOne == file.split(\".\")[0]:\n","                    print(\"\\n\\n### EXAMPLE ###\")\n","                    print(\"Savin example:\", img_name)\n","\n","                    edge_unique_labels = np.unique(el)\n","                    g.edata['label'] = torch.tensor([np.where(target == edge_unique_labels)[0][0] for target in el])\n","\n","                    g = self.balance_edges(g, 3, int(np.where('none' == edge_unique_labels)[0][0]))\n","\n","                    img_removed = Image.open(img_path).convert('RGB')\n","                    draw_removed = ImageDraw.Draw(img_removed)\n","\n","                    for b, box in enumerate(boxs):\n","                        if nl[b] == 'header':\n","                            color = 'yellow'\n","                        elif nl[b] == 'question':\n","                            color = 'blue'\n","                        elif nl[b] == 'answer':\n","                            color = 'green'\n","                        else:\n","                            color = 'gray'\n","                        draw_removed.rectangle(box, outline=color, width=3)\n","\n","                    u, v = g.all_edges()\n","                    labels = g.edata['label'].tolist()\n","                    u, v = u.tolist(), v.tolist()\n","\n","                    center = lambda rect: ((rect[2]+rect[0])/2, (rect[3]+rect[1])/2)\n","\n","                    num_pair = 0\n","                    num_none = 0\n","\n","                    for p, pair in enumerate(zip(u,v)):\n","                        sc = center(boxs[pair[0]])\n","                        ec = center(boxs[pair[1]])\n","                        if labels[p] == int(np.where('pair' == edge_unique_labels)[0][0]):\n","                            num_pair += 1\n","                            color = 'violet'\n","                            draw_removed.ellipse([(sc[0]-4,sc[1]-4), (sc[0]+4,sc[1]+4)], fill = 'green', outline='black')\n","                            draw_removed.ellipse([(ec[0]-4,ec[1]-4), (ec[0]+4,ec[1]+4)], fill = 'red', outline='black')\n","                        else:\n","                            num_none += 1\n","                            color='gray'\n","                        draw_removed.line((sc,ec), fill=color, width=3)\n","\n","                    print(\"Balanced Links: None {} | Key-Value {}\".format(num_none, num_pair))\n","                    img_removed.save(f'esempi/FUNSD/{img_name}_removed_graph.png')\n","\n","        elif self.node_granularity[0] == 'yolo':\n","            path_preds = os.path.join(src, 'yolo_bbox')\n","            path_images = os.path.join(src, 'images')\n","            path_gts = os.path.join(src, 'adjusted_annotations')\n","            all_paths, all_preds, all_links, all_labels, all_texts = load_predictions(path_preds, path_gts, path_images)\n","            for f, img_path in enumerate(tqdm(all_paths, desc='Creating graphs - YOLO')):\n","\n","                features['paths'].append(img_path)\n","                features['boxs'].append(all_preds[f])\n","                features['texts'].append(all_texts[f])\n","                node_labels.append(all_labels[f])\n","                pair_labels = all_links[f]\n","\n","                # getting edges\n","                if self.edge_type[0] == 'fully':\n","                    u, v = self.fully_connected(range(len(features['boxs'][f])))\n","                elif self.edge_type[0] == 'knn':\n","                    u,v = self.knn_connection(Image.open(img_path).size, features['boxs'][f])\n","                else:\n","                    raise Exception('GraphBuilder exception: Other edge types still under development.')\n","\n","                el = list()\n","                for e in zip(u, v):\n","                    edge = [e[0], e[1]]\n","                    if edge in pair_labels: el.append('pair')\n","                    else: el.append('none')\n","                edge_labels.append(el)\n","\n","                # creating graph\n","                g = dgl.graph((torch.tensor(u), torch.tensor(v)), num_nodes=len(features['boxs'][f]), idtype=torch.int32)\n","                graphs.append(g)\n","        else:\n","            #TODO develop OCR too\n","            raise Exception('GraphBuilder Exception: only \\'gt\\' or \\'yolo\\' available for FUNSD.')\n","\n","\n","        return graphs, node_labels, edge_labels, features"],"metadata":{"id":"Y8VoC_vTkJqK"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oIX7qyslQeXB"},"outputs":[],"source":["from pathlib import Path\n","CONFIGS = Path(\"/content/drive/MyDrive/doc2graph-master/configs\")\n","k = 3 #! try changing this value!\n","\n","gb = GraphBuilder()\n","u, v = gb.knn_connection(image.size, boxs, k)\n","links = {'src': u, 'dst': v}\n","draw_results(image, boxs, links)\n","image"]},{"cell_type":"code","source":["image_path = '/content/drive/MyDrive/doc2graph-master/tutorial/funsd/dataset/training_data'\n","image_path_test = '/content/drive/MyDrive/doc2graph-master/tutorial/funsd/dataset/testing_data'"],"metadata":{"id":"yG8P0wrp11LT"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dxpbP9iEQeXB"},"outputs":[],"source":["graphs, node_labels, edge_labels, features = gb.get_graph(image_path, 'FUNSD')\n","graphs_test, node_labels_test, edge_labels_test, features_test = gb.get_graph(image_path_test, 'FUNSD')"]},{"cell_type":"code","source":["graphs[0], graphs_test[0]"],"metadata":{"id":"K0lV4vaB253W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["node_labels[0]"],"metadata":{"id":"fOt28pg-woYP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install segmentation-models-pytorch"],"metadata":{"id":"3ZOcd65dw-MG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from segmentation_models_pytorch.base import modules as md\n","\n","\n","class DecoderBlock(nn.Module):\n","    def __init__(\n","            self,\n","            in_channels,\n","            skip_channels,\n","            out_channels,\n","            use_batchnorm=True,\n","            attention_type=None,\n","    ):\n","        super().__init__()\n","        self.conv1 = md.Conv2dReLU(\n","            in_channels + skip_channels,\n","            out_channels,\n","            kernel_size=3,\n","            padding=1,\n","            use_batchnorm=use_batchnorm,\n","        )\n","        self.attention1 = md.Attention(attention_type, in_channels=in_channels + skip_channels)\n","        self.conv2 = md.Conv2dReLU(\n","            out_channels,\n","            out_channels,\n","            kernel_size=3,\n","            padding=1,\n","            use_batchnorm=use_batchnorm,\n","        )\n","        self.attention2 = md.Attention(attention_type, in_channels=out_channels)\n","\n","    def forward(self, x, skip=None):\n","        if skip is not None:\n","            x = F.interpolate(x, size=skip.shape[2:], mode=\"nearest\")\n","            x = torch.cat([x, skip], dim=1)\n","            x = self.attention1(x)\n","        else:\n","            x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        x = self.attention2(x)\n","        return x\n","\n","\n","class CenterBlock(nn.Sequential):\n","    def __init__(self, in_channels, out_channels, use_batchnorm=True):\n","        conv1 = md.Conv2dReLU(\n","            in_channels,\n","            out_channels,\n","            kernel_size=3,\n","            padding=1,\n","            use_batchnorm=use_batchnorm,\n","        )\n","        conv2 = md.Conv2dReLU(\n","            out_channels,\n","            out_channels,\n","            kernel_size=3,\n","            padding=1,\n","            use_batchnorm=use_batchnorm,\n","        )\n","        super().__init__(conv1, conv2)\n","\n","\n","class UnetDecoder(nn.Module):\n","    def __init__(\n","            self,\n","            encoder_channels,\n","            decoder_channels,\n","            n_blocks=5,\n","            use_batchnorm=True,\n","            attention_type=None,\n","            center=False,\n","    ):\n","        super().__init__()\n","\n","        if n_blocks != len(decoder_channels):\n","            raise ValueError(\n","                \"Model depth is {}, but you provide `decoder_channels` for {} blocks.\".format(\n","                    n_blocks, len(decoder_channels)\n","                )\n","            )\n","\n","        encoder_channels = encoder_channels[1:]  # remove first skip with same spatial resolution\n","        encoder_channels = encoder_channels[::-1]  # reverse channels to start from head of encoder\n","\n","        # computing blocks input and output channels\n","        head_channels = encoder_channels[0]\n","        in_channels = [head_channels] + list(decoder_channels[:-1])\n","        skip_channels = list(encoder_channels[1:]) + [0]\n","        out_channels = decoder_channels\n","\n","        if center:\n","            self.center = CenterBlock(\n","                head_channels, head_channels, use_batchnorm=use_batchnorm\n","            )\n","        else:\n","            self.center = nn.Identity()\n","\n","        # combine decoder keyword arguments\n","        kwargs = dict(use_batchnorm=use_batchnorm, attention_type=attention_type)\n","        blocks = [\n","            DecoderBlock(in_ch, skip_ch, out_ch, **kwargs)\n","            for in_ch, skip_ch, out_ch in zip(in_channels, skip_channels, out_channels)\n","        ]\n","        self.blocks = nn.ModuleList(blocks)\n","\n","    def forward(self, *features):\n","\n","        features = features[1:]    # remove first skip with same spatial resolution\n","        features = features[::-1]  # reverse channels to start from head of encoder\n","\n","        head = features[0]\n","        skips = features[1:]\n","\n","        x = self.center(head)\n","        for i, decoder_block in enumerate(self.blocks):\n","            skip = skips[i] if i < len(skips) else None\n","            x = decoder_block(x, skip)\n","\n","        return x"],"metadata":{"id":"Y_KanT4v3wxI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from typing import Optional, Union, List\n","from segmentation_models_pytorch.encoders import get_encoder\n","from segmentation_models_pytorch.base import SegmentationModel\n","from segmentation_models_pytorch.base import SegmentationHead, ClassificationHead\n","\n","\n","class Unet(SegmentationModel):\n","    \"\"\"Unet_ is a fully convolution neural network for image semantic segmentation. Consist of *encoder*\n","    and *decoder* parts connected with *skip connections*. Encoder extract features of different spatial\n","    resolution (skip connections) which are used by decoder to define accurate segmentation mask. Use *concatenation*\n","    for fusing decoder blocks with skip connections.\n","\n","    Args:\n","        encoder_name: Name of the classification model that will be used as an encoder (a.k.a backbone)\n","            to extract features of different spatial resolution\n","        encoder_depth: A number of stages used in encoder in range [3, 5]. Each stage generate features\n","            two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features\n","            with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on).\n","            Default is 5\n","        encoder_weights: One of **None** (random initialization), **\"imagenet\"** (pre-training on ImageNet) and\n","            other pretrained weights (see table with available weights for each encoder_name)\n","        decoder_channels: List of integers which specify **in_channels** parameter for convolutions used in decoder.\n","            Length of the list should be the same as **encoder_depth**\n","        decoder_use_batchnorm: If **True**, BatchNorm2d layer between Conv2D and Activation layers\n","            is used. If **\"inplace\"** InplaceABN will be used, allows to decrease memory consumption.\n","            Available options are **True, False, \"inplace\"**\n","        decoder_attention_type: Attention module used in decoder of the model. Available options are **None** and **scse**.\n","            SCSE paper - https://arxiv.org/abs/1808.08127\n","        in_channels: A number of input channels for the model, default is 3 (RGB images)\n","        classes: A number of classes for output mask (or you can think as a number of channels of output mask)\n","        activation: An activation function to apply after the final convolution layer.\n","            Available options are **\"sigmoid\"**, **\"softmax\"**, **\"logsoftmax\"**, **\"tanh\"**, **\"identity\"**, **callable** and **None**.\n","            Default is **None**\n","        aux_params: Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build\n","            on top of encoder if **aux_params** is not **None** (default). Supported params:\n","                - classes (int): A number of classes\n","                - pooling (str): One of \"max\", \"avg\". Default is \"avg\"\n","                - dropout (float): Dropout factor in [0, 1)\n","                - activation (str): An activation function to apply \"sigmoid\"/\"softmax\" (could be **None** to return logits)\n","\n","    Returns:\n","        ``torch.nn.Module``: Unet\n","\n","    .. _Unet:\n","        https://arxiv.org/abs/1505.04597\n","\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        encoder_name: str = \"resnet34\",\n","        encoder_depth: int = 5,\n","        encoder_weights: Optional[str] = \"imagenet\",\n","        decoder_use_batchnorm: bool = True,\n","        decoder_channels: List[int] = (256, 128, 64, 32, 16),\n","        decoder_attention_type: Optional[str] = None,\n","        in_channels: int = 3,\n","        classes: int = 1,\n","        activation: Optional[Union[str, callable]] = None,\n","        aux_params: Optional[dict] = None,\n","    ):\n","        super().__init__()\n","\n","        self.encoder = get_encoder(\n","            encoder_name,\n","            in_channels=in_channels,\n","            depth=encoder_depth,\n","            weights=encoder_weights,\n","        )\n","\n","        self.decoder = UnetDecoder(\n","            encoder_channels=self.encoder.out_channels,\n","            decoder_channels=decoder_channels,\n","            n_blocks=encoder_depth,\n","            use_batchnorm=decoder_use_batchnorm,\n","            center=True if encoder_name.startswith(\"vgg\") else False,\n","            attention_type=decoder_attention_type,\n","        )\n","\n","        self.segmentation_head = SegmentationHead(\n","            in_channels=decoder_channels[-1],\n","            out_channels=classes,\n","            activation=activation,\n","            kernel_size=3,\n","        )\n","\n","        if aux_params is not None:\n","            self.classification_head = ClassificationHead(\n","                in_channels=self.encoder.out_channels[-1], **aux_params\n","            )\n","        else:\n","            self.classification_head = None\n","\n","        self.name = \"u-{}\".format(encoder_name)\n","        self.initialize()"],"metadata":{"id":"SoWL-a1Q4D1X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!unzip /content/drive/MyDrive/doc2graph-master/tutorial/checkpoints.zip -d /content/drive/MyDrive/doc2graph-master/tutorial/funsd"],"metadata":{"id":"8q_OyThw7Mdv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","from typing import Tuple\n","import spacy\n","import torch\n","import torchvision\n","from tqdm import tqdm\n","from PIL import Image, ImageDraw\n","import torchvision.transforms.functional as tvF\n","CHECKPOINTS = Path('/content/drive/MyDrive/doc2graph-master/tutorial/funsd/checkpoints')\n","class FeatureBuilder():\n","\n","    def __init__(self, d : int = 'cpu'):\n","        \"\"\"FeatureBuilder constructor\n","\n","        Args:\n","            d (int): device number, if any (cpu or cuda:n)\n","        \"\"\"\n","        self.cfg_preprocessing = get_config('base')\n","        self.device = d\n","        self.add_geom = self.cfg_preprocessing.FEATURES.add_geom\n","        self.add_embs = self.cfg_preprocessing.FEATURES.add_embs\n","        self.add_hist = self.cfg_preprocessing.FEATURES.add_hist\n","        self.add_visual = self.cfg_preprocessing.FEATURES.add_visual\n","        self.add_eweights = self.cfg_preprocessing.FEATURES.add_eweights\n","        self.add_fudge = self.cfg_preprocessing.FEATURES.add_fudge\n","        self.num_polar_bins = self.cfg_preprocessing.FEATURES.num_polar_bins\n","\n","        if self.add_embs:\n","            self.text_embedder = spacy.load('en_core_web_lg')\n","\n","        if self.add_visual:\n","            self.visual_embedder = Unet(encoder_name=\"mobilenet_v2\", encoder_weights=None, in_channels=1, classes=4)\n","            self.visual_embedder.load_state_dict(torch.load(CHECKPOINTS / 'backbone_unet.pth')['weights'])\n","            self.visual_embedder = self.visual_embedder.encoder\n","            self.visual_embedder.to(d)\n","\n","        self.sg = lambda rect, s : [rect[0]/s[0], rect[1]/s[1], rect[2]/s[0], rect[3]/s[1]] # scaling by img width and height\n","\n","    def add_features(self, graphs : list, features : list) -> Tuple[list, int]:\n","        \"\"\" Add features to provided graphs\n","\n","        Args:\n","            graphs (list) : list of DGLGraphs\n","            features (list) : list of features \"sources\", like text, positions and images\n","\n","        Returns:\n","            chunks list and its lenght\n","        \"\"\"\n","\n","        for id, g in enumerate(tqdm(graphs, desc='adding features')):\n","\n","            # positional features\n","            size = Image.open(features['paths'][id]).size\n","            feats = [[] for _ in range(len(features['boxs'][id]))]\n","            geom = [self.sg(box, size) for box in features['boxs'][id]]\n","            chunks = []\n","\n","            # 'geometrical' features\n","            if self.add_geom:\n","\n","                # TODO add 2d encoding like \"LayoutLM*\"\n","                [feats[idx].extend(self.sg(box, size)) for idx, box in enumerate(features['boxs'][id])]\n","                chunks.append(4)\n","\n","            # HISTOGRAM OF TEXT\n","            if self.add_hist:\n","\n","                [feats[idx].extend(hist) for idx, hist in enumerate(get_histogram(features['texts'][id]))]\n","                chunks.append(4)\n","\n","            # textual features\n","            if self.add_embs:\n","\n","                # LANGUAGE MODEL (SPACY)\n","                [feats[idx].extend(self.text_embedder(features['texts'][id][idx]).vector) for idx, _ in enumerate(feats)]\n","                chunks.append(len(self.text_embedder(features['texts'][id][0]).vector))\n","\n","            # visual features\n","            # https://pytorch.org/vision/stable/generated/torchvision.ops.roi_align.html?highlight=roi\n","            if self.add_visual:\n","                img = Image.open(features['paths'][id])\n","                visual_emb = self.visual_embedder(tvF.to_tensor(img).unsqueeze_(0).to(self.device)) # output [batch, channels, dim1, dim2]\n","                bboxs = [torch.Tensor(b) for b in features['boxs'][id]]\n","                bboxs = [torch.stack(bboxs, dim=0).to(self.device)]\n","                h = [torchvision.ops.roi_align(input=ve, boxes=bboxs, spatial_scale=1/ min(size[1] / ve.shape[2] , size[0] / ve.shape[3]), output_size=1) for ve in visual_emb[1:]]\n","                h = torch.cat(h, dim=1)\n","\n","                # VISUAL FEATURES (RESNET-IMAGENET)\n","                [feats[idx].extend(torch.flatten(h[idx]).tolist()) for idx, _ in enumerate(feats)]\n","                chunks.append(len(torch.flatten(h[0]).tolist()))\n","\n","            if self.add_eweights:\n","                u, v = g.edges()\n","                srcs, dsts =  u.tolist(), v.tolist()\n","                distances = []\n","                angles = []\n","\n","                # TODO CHOOSE WHICH DISTANCE NORMALIZATION TO APPLY\n","                #! with fully connected simply normalized with max distance between distances\n","                # m = sqrt((size[0]*size[0] + size[1]*size[1]))\n","                # parable = lambda x : (-x+1)**4\n","\n","                for pair in zip(srcs, dsts):\n","                    dist, angle = polar(features['boxs'][id][pair[0]], features['boxs'][id][pair[1]])\n","                    distances.append(dist)\n","                    angles.append(angle)\n","\n","                m = max(distances)\n","                polar_coordinates = to_bin(distances, angles, self.num_polar_bins)\n","                g.edata['feat'] = polar_coordinates\n","\n","            else:\n","                distances = ([0.0 for _ in range(g.number_of_edges())])\n","                m = 1\n","\n","            g.ndata['geom'] = torch.tensor(geom, dtype=torch.float32)\n","            g.ndata['feat'] = torch.tensor(feats, dtype=torch.float32)\n","\n","            distances = torch.tensor([(1-d/m) for d in distances], dtype=torch.float32)\n","            tresh_dist = torch.where(distances > 0.9, torch.full_like(distances, 0.1), torch.zeros_like(distances))\n","            g.edata['weights'] = tresh_dist\n","\n","            norm = []\n","            num_nodes = len(features['boxs'][id]) - 1\n","            for n in range(num_nodes + 1):\n","                neigs = torch.count_nonzero(tresh_dist[n*num_nodes:(n+1)*num_nodes]).tolist()\n","                try: norm.append([1. / neigs])\n","                except: norm.append([1.])\n","            g.ndata['norm'] = torch.tensor(norm, dtype=torch.float32)\n","\n","            #! DEBUG PURPOSES TO VISUALIZE RANDOM GRAPH IMAGE FROM DATASET\n","            if False:\n","                if id == rand_id and self.add_eweights:\n","                    print(\"\\n\\n### EXAMPLE ###\")\n","\n","                    img_path = features['paths'][id]\n","                    img = Image.open(img_path).convert('RGB')\n","                    draw = ImageDraw.Draw(img)\n","\n","                    center = lambda rect: ((rect[2]+rect[0])/2, (rect[3]+rect[1])/2)\n","                    select = [random.randint(0, len(srcs)) for _ in range(10)]\n","                    for p, pair in enumerate(zip(srcs, dsts)):\n","                        if p in select:\n","                            sc = center(features['boxs'][id][pair[0]])\n","                            ec = center(features['boxs'][id][pair[1]])\n","                            draw.line((sc, ec), fill='grey', width=3)\n","                            middle_point = ((sc[0] + ec[0])/2,(sc[1] + ec[1])/2)\n","                            draw.text(middle_point, str(angles[p]), fill='black')\n","                            draw.rectangle(features['boxs'][id][pair[0]], fill='red')\n","                            draw.rectangle(features['boxs'][id][pair[1]], fill='blue')\n","\n","                    img.save(f'esempi/FUNSD/edges.png')\n","\n","        return chunks, len(chunks)\n","\n","    def get_info(self):\n","        print(f\"-> textual feats: {self.add_embs}\\n-> visual feats: {self.add_visual}\\n-> edge feats: {self.add_eweights}\")"],"metadata":{"id":"QuH7Ko3Q3OTs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python -m spacy download en_core_web_lg"],"metadata":{"id":"IqG5_e3M5ch5"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gjirldb2QeXB"},"outputs":[],"source":["device = 'cuda:0'\n","fb = FeatureBuilder(d=device)\n","chunks, _ = fb.add_features(graphs, features) # chunks is used by the model to merge different embeddings together!"]},{"cell_type":"code","source":["chunks_test, _ = fb.add_features(graphs_test, features_test)"],"metadata":{"id":"-z7tUbmzwrkV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["graphs"],"metadata":{"id":"CC2YrmuLykme"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["node_unique_labels = np.unique(np.array([l for nl in node_labels for l in nl]))\n","node_num_classes = len(node_unique_labels)\n","node_num_features = graphs[0].ndata['feat'].shape[1]\n","\n","for idx, labels in enumerate(node_labels):\n","  graphs[idx].ndata['label'] = torch.tensor([np.where(target == node_unique_labels)[0][0] for target in labels], dtype=torch.int64)"],"metadata":{"id":"E1XAHTMynuha"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_aQRr-dUoE7h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["node_unique_labels_test = np.unique(np.array([l for nl in node_labels_test for l in nl]))\n","node_num_classes_test = len(node_unique_labels_test)\n","node_num_features_test = graphs_test[0].ndata['feat'].shape[1]\n","\n","for idx, labels in enumerate(node_labels_test):\n","  graphs_test[idx].ndata['label'] = torch.tensor([np.where(target == node_unique_labels_test)[0][0] for target in labels], dtype=torch.int64)"],"metadata":{"id":"K_SgfwoQw0Jy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["edge_unique_labels = np.unique(edge_labels[0])\n","edge_num_classes = len(edge_unique_labels)\n","try:\n","  edge_num_features = graphs[0].edata['feat'].shape[1]\n","except:\n","  edge_num_features = 0\n","\n","for idx, labels in enumerate(edge_labels):\n","  graphs[idx].edata['label'] = torch.tensor([np.where(target == edge_unique_labels)[0][0] for target in labels], dtype=torch.int64)"],"metadata":{"id":"KKlrG8QOoE-E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["edge_unique_labels_test = np.unique(edge_labels_test[0])\n","edge_num_classes_test = len(edge_unique_labels_test)\n","try:\n","  edge_num_features_test = graphs_test[0].edata['feat'].shape[1]\n","except:\n","  edge_num_features_test = 0\n","\n","for idx, labels in enumerate(edge_labels_test):\n","  graphs_test[idx].edata['label'] = torch.tensor([np.where(target == edge_unique_labels_test)[0][0] for target in labels], dtype=torch.int64)\n"],"metadata":{"id":"VRevubtJxL51"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["graphs[0]"],"metadata":{"id":"wv5qU75M8t2K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install torch-geometric"],"metadata":{"id":"5SoBVryBg_fX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch_geometric.data import Data\n","\n","def dgl_to_pyg(graph):\n","    num_nodes = graph.num_nodes()\n","    num_edges = graph.num_edges()\n","    x = graph.ndata['feat']\n","    edge_attr = graph.edata['feat']\n","    src, dst = graph.edges()\n","    edge_index = torch.stack([src, dst], dim=0)\n","    y = graph.ndata['label']\n","    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n","\n","    return data\n","pyg_graphs = []\n","for dgl_graph in graphs:\n","    pyg_graph = dgl_to_pyg(dgl_graph)\n","    pyg_graphs.append(pyg_graph)"],"metadata":{"id":"HOWf8SdBhitE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pyg_graphs_test = []\n","for dgl_graph in graphs_test:\n","    pyg_graph = dgl_to_pyg(dgl_graph)\n","    pyg_graphs_test.append(pyg_graph)"],"metadata":{"id":"b8kFvs7WlqSe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pyg_graphs"],"metadata":{"id":"qHe3VsyIi3Ee"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ConGAT model"],"metadata":{"id":"oU9arft2OPD7"}},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","from torch_geometric.data import Data, DataLoader\n","from torch_geometric.nn import GATConv\n","\n","class GAT(torch.nn.Module):\n","    def __init__(self, in_channels, hidden_channels, out_channels, num_heads, dropout):\n","        super(GAT, self).__init__()\n","        self.conv1 = GATConv(in_channels, hidden_channels, heads=num_heads)\n","        self.dropout1 = torch.nn.Dropout(p=dropout)\n","        self.conv2 = GATConv(hidden_channels * num_heads, hidden_channels, heads=num_heads)\n","        self.dropout2 = torch.nn.Dropout(p=dropout)\n","        self.conv3 = GATConv(hidden_channels * num_heads, hidden_channels, heads=num_heads)\n","        self.dropout3 = torch.nn.Dropout(p=dropout)\n","        self.conv4 = GATConv(hidden_channels * num_heads, out_channels, heads=num_heads)\n","\n","    def forward(self, data):\n","        x, edge_index = data.x, data.edge_index\n","\n","        x = F.elu(self.conv1(x, edge_index))\n","        x = self.dropout1(x)\n","        x = F.elu(self.conv2(x, edge_index))\n","        x = self.dropout2(x)\n","        x = F.elu(self.conv3(x, edge_index))\n","        x = self.dropout3(x)\n","        x = F.elu(self.conv4(x, edge_index))\n","\n","        return x\n","\n","loader = DataLoader(pyg_graphs, batch_size=1, shuffle=True)\n","model = GAT(in_channels=1752, hidden_channels=8, out_channels=4, num_heads=4, dropout=0.2)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.9)"],"metadata":{"id":"87IVtqipjZzY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.train()\n","for epoch in range(2000):\n","    for data in loader:\n","        optimizer.zero_grad()\n","        out = model(data)\n","        loss = F.cross_entropy(out, data.y)  # Loss function được thay đổi để phù hợp với số lớp\n","        loss.backward()\n","        optimizer.step()\n","\n","    print(f'Epoch {epoch}, Loss: {loss.item()}')"],"metadata":{"id":"wNxZiV4iuze5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loader_test = DataLoader(pyg_graphs_test, batch_size=1, shuffle=True)"],"metadata":{"id":"lI5eAkTLmJwS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from sklearn.metrics import accuracy_score, f1_score\n","\n","def evaluate_model(model, data_loader):\n","    model.eval()\n","    predictions = []\n","    targets = []\n","\n","    with torch.no_grad():\n","        for data in data_loader:\n","            out = model(data)\n","            _, predicted = torch.max(out, 1)\n","            predictions.extend(predicted.tolist())\n","            targets.extend(data.y.tolist())\n","\n","    accuracy = accuracy_score(targets, predictions)\n","    f1 = f1_score(targets, predictions, average='weighted')\n","\n","    return accuracy, f1\n","\n","accuracy, f1 = evaluate_model(model, loader_test)\n","print(f'Accuracy: {accuracy}, F1 Score: {f1}')"],"metadata":{"id":"tGKiMyVvjZ19"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"esFBp-ALAPEr"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"orig_nbformat":4,"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["0CILJ4NLOFvl"]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}